# Evaluation of Vision-Language Models

In our project, we aim to assess the performance of Vision-Language Pre-training (VLP) models, BLIP. I evaluated BLIP VQA architecture on fine-grained datasets.
We start by conducting zero-shot evaluations to understand its performance without any finetuning and then proceed to fine-tune the model on the selected datasets. 
We employ a range of evaluation metrics commonly used in both vision and natural language processing domains to ensure a comprehensive assessment and obtain accurate inferences from the models. 

During the evaluation of Vision Language models for Visual Question Answering, we utilize the Caltech-UCSD Birds-200-2011 (CUB-200-2011) datase
